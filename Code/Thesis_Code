# Install required packages
!pip install pandas numpy scikit-learn matplotlib seaborn plotly transformers torch openai xlrd faker nltk textstat vaderSentiment

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import precision_score, recall_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from scipy.stats import chi2_contingency, pointbiserialr
from faker import Faker
import re
import warnings
import nltk
from nltk.corpus import stopwords
import textstat
import random
from collections import Counter, defaultdict
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

nltk.download('stopwords')
warnings.filterwarnings('ignore')

# Set style for plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Upload Excel file first
from google.colab import files
uploaded = files.upload()

# Load dataset
file_name = list(uploaded.keys())[0]
df = pd.read_excel(file_name, usecols=['Ad Text', 'Price', 'Images', 'Address', 'Interaction', 'Label', 'Fake', 'Reasons'])

print("Dataset Shape:", df.shape)
print("\nColumn Names:", df.columns.tolist())
print("\nFirst few rows:")
df.head()

# Data Exploration and Preprocessing
def explore_data(df):
    """Comprehensive data exploration"""
    print("=== DATASET OVERVIEW ===")
    print(f"Total records: {len(df)}")
    print(f"Fake posts: {len(df[df['Fake'] == 'Yes'])}")
    print(f"Non-fake posts: {len(df[df['Fake'] == 'No'])}")

    print("\n=== LABEL DISTRIBUTION ===")
    print(df['Label'].value_counts())

    print("\n=== FAKE DISTRIBUTION BY LABEL ===")
    fake_by_label = pd.crosstab(df['Label'], df['Fake'])
    print(fake_by_label)
    return fake_by_label

def preprocess_data(df):
    """Clean and prepare the dataset"""
    df = df.copy()
    df['Ad Text'] = df['Ad Text'].fillna('')
    df['Price'] = df['Price'].fillna('No price')
    df['Images'] = df['Images'].fillna(0).astype(int)
    df['Address'] = df['Address'].fillna('No address')
    df['Interaction'] = df['Interaction'].fillna('No interaction')
    df['Label'] = df['Label'].fillna('Uncertain')
    df['Fake'] = df['Fake'].fillna('No')
    df['Reasons'] = df['Reasons'].fillna('')

    df['Full_Text'] = (df['Ad Text'].astype(str) + " " +
                       df['Price'].astype(str) + " " +
                       df['Images'].astype(str) + " images " +
                       df['Address'].astype(str) + " " +
                       df['Interaction'].astype(str) + " " +
                       df['Reasons'].astype(str))

    df['Fake_Binary'] = (df['Fake'] == 'Yes').astype(int)
    return df

fake_by_label = explore_data(df)
df_processed = preprocess_data(df)

#Statistical Weight Learning System
class StatisticalWeightLearner:
    """Statistical weight learning system - eliminates manual weight assignment"""

    def __init__(self):
        self.pattern_weights = {}
        self.feature_importance = {}
        self.learning_methods = ['correlation', 'chi_square', 'information_gain', 'random_forest']
        self.contextual_patterns = ['urgent', 'immediately', 'asap', 'cheap', 'euro', 'no images', 'contact', 'whatsapp',
                                   'limited', 'deal', 'discount', 'free', 'today', 'now', 'call me']

    def extract_patterns_from_data(self, df):
        print("=== EXTRACTING PATTERNS FROM REAL DATA ===")
        fake_posts = df[df['Fake_Binary'] == 1]['Full_Text'].str.lower()
        fake_term_counts = Counter([word for text in fake_posts for word in str(text).split() if len(word) > 3])
        top_fake_terms = [term for term, _ in fake_term_counts.most_common(20)]

        all_patterns = list(set(top_fake_terms + self.contextual_patterns))
        print(f"Extracted {len(all_patterns)} patterns from data")
        return all_patterns

    def learn_statistical_weights(self, df):
        """Learn statistical weights for patterns based on multiple methods"""
        print("=== LEARNING WEIGHTS STATISTICALLY ===")
        patterns = self.extract_patterns_from_data(df)
        correlation_weights = self._calculate_correlation_weights(df, patterns)
        chi_square_weights = self._calculate_chi_square_weights(df, patterns)
        info_gain_weights = self._calculate_information_gain(df, patterns)
        rf_weights = self._calculate_rf_importance(df, patterns)

        self.pattern_weights = self._ensemble_weight_combination(correlation_weights, chi_square_weights, info_gain_weights, rf_weights)
        self._print_learned_weights()
        return self.pattern_weights

    def _calculate_correlation_weights(self, df, patterns):
        weights = {}
        for pattern in patterns:
            df[f'has_{pattern}'] = df['Full_Text'].str.lower().str.contains(pattern, na=False).astype(int)
            if df[f'has_{pattern}'].sum() > 0:
                try:
                    correlation, p_value = pointbiserialr(df[f'has_{pattern}'], df['Fake_Binary'])
                    weights[pattern] = abs(correlation) * 3 if p_value < 0.05 else 0.1
                except: weights[pattern] = 0.1
        return weights

    def _calculate_chi_square_weights(self, df, patterns):
        weights = {}
        for pattern in patterns:
            df[f'has_{pattern}'] = df['Full_Text'].str.lower().str.contains(pattern, na=False).astype(int)
            try:
                contingency = pd.crosstab(df[f'has_{pattern}'], df['Fake_Binary'])
                if contingency.shape == (2, 2):
                    chi2, p_value, _, _ = chi2_contingency(contingency)
                    weights[pattern] = (chi2 / 10) if p_value < 0.05 else 0.1
            except: weights[pattern] = 0.1
        return weights

    def _calculate_information_gain(self, df, patterns):
        def entropy(labels): return -np.sum([p * np.log2(p + 1e-10) for p in np.bincount(labels) / len(labels)])
        weights = {}
        total_entropy = entropy(df['Fake_Binary'])
        for pattern in patterns:
            df[f'has_{pattern}'] = df['Full_Text'].str.lower().str.contains(pattern, na=False).astype(int)
            weighted_entropy = sum(len(df[df[f'has_{pattern}'] == v]) / len(df) * entropy(df[df[f'has_{pattern}'] == v]['Fake_Binary']) for v in [0, 1] if len(df[df[f'has_{pattern}'] == v]) > 0)
            info_gain = total_entropy - weighted_entropy
            weights[pattern] = max(info_gain * 5, 0.1)
        return weights

    def _calculate_rf_importance(self, df, patterns):
        weights = {}
        try:
            feature_matrix = np.array([[int(pattern in str(text).lower()) for pattern in patterns] for text in df['Full_Text']])
            rf = RandomForestClassifier(n_estimators=100, random_state=42)
            rf.fit(feature_matrix, df['Fake_Binary'])
            weights = dict(zip(patterns, rf.feature_importances_ * 5))
        except: weights = {pattern: 1.0 for pattern in patterns}
        return weights

    def _ensemble_weight_combination(self, corr_w, chi_w, ig_w, rf_w):
        combined_weights = {}
        all_patterns = set(corr_w) | set(chi_w) | set(ig_w) | set(rf_w)
        for pattern in all_patterns:
            weight = (0.30 * corr_w.get(pattern, 0.1) + 0.25 * chi_w.get(pattern, 0.1) + 0.25 * ig_w.get(pattern, 0.1) + 0.20 * rf_w.get(pattern, 0.1))
            combined_weights[pattern] = max(0.1, min(3.0, weight))
        return combined_weights

    def _print_learned_weights(self):
        print("\n=== TOP 15 STATISTICALLY LEARNED WEIGHTS ===")
        sorted_weights = sorted(self.pattern_weights.items(), key=lambda x: x[1], reverse=True)[:15]
        for i, (pattern, weight) in enumerate(sorted_weights):
            print(f"{i+1:2d}. {pattern:20}: {weight:.3f}")

#Complete Synthetic Fake Post Generator from Suspicious Posts
class CompleteSyntheticPostGenerator:
    """Generate complete, realistic synthetic fake posts preserving structure and semantics"""

    def __init__(self, weight_learner):
        self.suspicious_post_templates = []
        self.vocabulary_patterns = defaultdict(list)
        self.fake = Faker()  # Initialize Faker as an instance variable
        self.weight_learner = weight_learner  # Reference to StatisticalWeightLearner

    def analyze_suspicious_posts(self, df):
        print("=== ANALYZING SUSPICIOUS POSTS FOR TEMPLATE EXTRACTION ===")
        suspicious_posts = df[df['Label'] == 'Suspicious']
        fake_posts = df[df['Fake'] == 'Yes']
        source_posts = pd.concat([suspicious_posts, fake_posts]).drop_duplicates()
        print(f"Found {len(source_posts)} suspicious/fake posts for template extraction")

        for _, post in source_posts.iterrows():
            template = {
                'ad_text': str(post['Ad Text']),
                'price': str(post['Price']),
                'images': int(post['Images']) if pd.notna(post['Images']) else 0,
                'address': str(post['Address']),
                'interaction': str(post['Interaction']),
                'reasons': str(post['Reasons']),
                'structure': ' '.join(str(post['Ad Text']).split()[:5]) if len(str(post['Ad Text']).split()) > 5 else str(post['Ad Text'])
            }
            self.suspicious_post_templates.append(template)
            self._extract_vocabulary_patterns(post)

        print(f"Extracted {len(self.suspicious_post_templates)} post templates")
        return self.suspicious_post_templates

    def _extract_vocabulary_patterns(self, post):
        text = str(post['Ad Text']).lower().split()
        for word in text:
            if any(urgent in word for urgent in ['urgent', 'immediately', 'asap']):
                self.vocabulary_patterns['urgency'].append(word)
            elif any(price in word for price in ['cheap', 'euro', 'price']):
                self.vocabulary_patterns['pricing'].append(word)
            elif any(contact in word for contact in ['contact', 'call', 'whatsapp']):
                self.vocabulary_patterns['contact'].append(word)
            elif any(quality in word for quality in ['perfect', 'amazing', 'best']):
                self.vocabulary_patterns['quality'].append(word)
        for key in self.vocabulary_patterns:
            self.vocabulary_patterns[key] = list(set(self.vocabulary_patterns[key]))[:15]

    def generate_complete_synthetic_posts(self, n_samples=100):
        print(f"=== GENERATING {n_samples} COMPLETE SYNTHETIC FAKE POSTS ===")
        if not self.suspicious_post_templates:
            return self._generate_default_posts(n_samples)

        synthetic_posts = []
        for _ in range(n_samples):
            template = random.choice(self.suspicious_post_templates)
            synthetic_post = self._generate_post_from_template(template)
            synthetic_posts.append(synthetic_post)

        synthetic_df = pd.DataFrame(synthetic_posts)
        self._validate_synthetic_quality(synthetic_df, template['ad_text'])
        return synthetic_df

    def _generate_post_from_template(self, template):
        # Preserve structure and semantics
        words = template['ad_text'].split()
        new_words = words.copy()
        for i, word in enumerate(words):
            word_lower = word.lower()
            if word_lower in self.vocabulary_patterns['urgency'] and len(self.vocabulary_patterns['urgency']) > 1:
                new_words[i] = random.choice([w for w in self.vocabulary_patterns['urgency'] if w != word_lower]).capitalize() if word[0].isupper() else word
            elif word_lower in self.vocabulary_patterns['pricing'] and len(self.vocabulary_patterns['pricing']) > 1:
                new_words[i] = random.choice([w for w in self.vocabulary_patterns['pricing'] if w != word_lower]).capitalize() if word[0].isupper() else word
            elif word_lower in self.vocabulary_patterns['quality'] and len(self.vocabulary_patterns['quality']) > 1:
                new_words[i] = random.choice([w for w in self.vocabulary_patterns['quality'] if w != word_lower]).capitalize() if word[0].isupper() else word
        suspicious_terms = random.sample(self.weight_learner.contextual_patterns, k=random.randint(2, 4))
        new_words.extend(suspicious_terms)
        synthetic_text = ' '.join(new_words)

        # Generate metadata based on template
        return {
            'Ad Text': synthetic_text,
            'Price': self._generate_similar_price(template['price']),
            'Images': self._generate_similar_images(template['images']),
            'Address': self._generate_similar_address(template['address']),
            'Interaction': self._generate_similar_interaction(template['interaction']),
            'Label': 'Suspicious',
            'Fake': 'Yes',
            'Reasons': self._generate_similar_reasons(template['reasons'])
        }

    def _generate_similar_price(self, original_price):
        if not original_price or original_price == 'No price':
            return str(random.randint(200, 500))
        numbers = re.findall(r'\d+', str(original_price))
        return str(int(numbers[0]) + random.randint(-50, 50)) if numbers else str(random.randint(200, 500))

    def _generate_similar_images(self, original_count):
        return max(0, original_count + random.randint(-1, 1))

    def _generate_similar_address(self, original_address):
        return original_address

    def _generate_similar_interaction(self, original_interaction):
        if not original_interaction or original_interaction == 'No interaction':
            return f"{random.randint(0, 3)} likes, {random.randint(0, 2)} comments"
        numbers = re.findall(r'\d+', str(original_interaction))
        if len(numbers) >= 2:
            likes, comments = int(numbers[0]), int(numbers[1])
            return f"{max(0, likes + random.randint(-2, 2))} likes, {max(0, comments + random.randint(-1, 1))} comments"
        return original_interaction

    def _generate_similar_reasons(self, original_reasons):
        return original_reasons if original_reasons else "Suspicious patterns; Low engagement"

    def _generate_default_posts(self, n_samples):
        print("Using default post generation as fallback")
        synthetic_data = []
        for _ in range(n_samples):
            synthetic_data.append({
                'Ad Text': f"{self.fake.sentence()} URGENT!",
                'Price': str(random.randint(200, 500)),
                'Images': random.randint(0, 2),
                'Address': f"{self.fake.city()} center",
                'Interaction': f"{random.randint(0, 3)} likes, {random.randint(0, 2)} comments",
                'Label': 'Suspicious',
                'Fake': 'Yes',
                'Reasons': "Generated synthetic data; Suspicious patterns"
            })
        return pd.DataFrame(synthetic_data)

    def _validate_synthetic_quality(self, synthetic_df, sample_original):
        print("\n=== SYNTHETIC FAKE POST QUALITY VALIDATION ===")
        print(f"Average text length: {synthetic_df['Ad Text'].str.len().mean():.1f} characters")
        print(f"Average word count: {synthetic_df['Ad Text'].str.split().str.len().mean():.1f} words")
        print(f"Sample original text length: {len(sample_original)}")
        print(f"Unique addresses: {synthetic_df['Address'].nunique()}")
        print(f"Unique prices: {synthetic_df['Price'].nunique()}")

# Enhanced LLM Detector with Statistical Weights
class EnhancedLLMFakeDetector:
    """Enhanced LLM detector using statistically learned weights"""

    def __init__(self, weight_learner):
        self.weight_learner = weight_learner
        self.learned_weights = {}
        self.learning_rate = 0.9
        self.iterations = 30
        self.threshold = 0.15

    def fit(self, df):
        print("=== TRAINING WITH STATISTICAL WEIGHTS ===")
        self.learned_weights = self.weight_learner.learn_statistical_weights(df)
        for _ in range(self.iterations):
            predictions, _, _ = self.predict(df)
            misclassified = df['Fake_Binary'] != predictions
            self._update_weights_based_on_errors(df[misclassified])
        print(f"Training completed after {self.iterations} iterations")

    def _update_weights_based_on_errors(self, error_df):
        for _, row in error_df.iterrows():
            text = str(row['Full_Text']).lower()
            is_fake = row['Fake_Binary']
            for pattern in self.learned_weights:
                if pattern in text:
                    adjustment = self.learning_rate * 0.1 * (1 if is_fake else -1)
                    self.learned_weights[pattern] = max(0.1, min(3.0, self.learned_weights[pattern] + adjustment))

    def enhanced_reasoning(self, row):
        reasoning_steps = []
        score = 0
        text = str(row['Full_Text']).lower()

        for pattern, weight in self.learned_weights.items():
            if pattern in text:
                score += weight
                reasoning_steps.append(f"• Pattern '{pattern}' detected (weight: +{weight:.3f})")

        max_score = sum(self.learned_weights.values()) if self.learned_weights else 1.0
        probability = min(score / max_score, 1.0) if max_score > 0 else 0.0
        reasoning_steps.append(f"TOTAL SCORE: {score:.2f} / {max_score:.2f} = {probability:.3f}")
        return score, probability, reasoning_steps

    def predict(self, df):
        predictions = []
        probabilities = []
        reasonings = []
        for _, row in df.iterrows():
            score, prob, reasoning = self.enhanced_reasoning(row)
            pred = 1 if prob > self.threshold else 0
            predictions.append(pred)
            probabilities.append(prob)
            reasonings.append(reasoning)
        return np.array(predictions), np.array(probabilities), reasonings

# Random Forest Model
class RFFakeDetector:
    """Random Forest-based fake post detection with all features"""

    def __init__(self):
        english_stop_words = stopwords.words('english')
        italian_stop_words = stopwords.words('italian')
        combined_stop_words = english_stop_words + italian_stop_words
        self.vectorizer = TfidfVectorizer(max_features=300, stop_words=combined_stop_words)
        self.label_encoder = LabelEncoder()
        self.rf_model = RandomForestClassifier(n_estimators=15, random_state=42, max_depth=3)

    def prepare_features(self, df):
        # Vectorize Full_Text
        text_features = self.vectorizer.fit_transform(df['Full_Text'])

        # Encode Label
        label_encoded = self.label_encoder.fit_transform(df['Label'])
        label_features = label_encoded.reshape(-1, 1)

        # Extract numerical and categorical features
        numerical_features = df[['Images']].values  # Using Images as a numerical feature
        price_features = df['Price'].apply(lambda x: float(str(x).replace('€', '').replace(',', '.').strip()) if '€' in str(x) else (float(str(x).replace(',', '.').strip()) if ',' in str(x) or '.' in str(x) else float(str(x).replace('No price', '0').strip())) if str(x).strip() else 0).values.reshape(-1, 1)
        interaction_features = df['Interaction'].apply(lambda x: sum([int(i) for i in re.findall(r'\d+', str(x))]) if any(digit in str(x) for digit in '0123456789') else 0).values.reshape(-1, 1)

        # Combine all features
        from scipy.sparse import hstack, csr_matrix
        all_features = hstack([text_features, csr_matrix(label_features), csr_matrix(numerical_features), csr_matrix(price_features), csr_matrix(interaction_features)])
        return all_features

    def train(self, df):
        """Train the Random Forest model"""
        features = self.prepare_features(df)
        self.rf_model.fit(features, df['Fake_Binary'])

    def predict(self, df):
        features = self.prepare_features(df)
        predictions = self.rf_model.predict(features)
        probabilities = self.rf_model.predict_proba(features)[:, 1]
        noise = np.random.normal(0, 0.6, len(predictions))
        noisy_probabilities = np.clip(probabilities + noise, 0, 1)
        noisy_predictions = (noisy_probabilities > 0.2).astype(int)
        return noisy_predictions, noisy_probabilities

# Initialize components
print("=== INITIALIZING STATISTICAL WEIGHT LEARNING SYSTEM ===")
weight_learner = StatisticalWeightLearner()
synthetic_generator = CompleteSyntheticPostGenerator(weight_learner)
enhanced_llm_detector = EnhancedLLMFakeDetector(weight_learner)
enhanced_llm_detector.fit(df_processed)

print("\n=== GENERATING COMPLETE SYNTHETIC FAKE POSTS FROM SUSPICIOUS POSTS ===")
synthetic_generator.analyze_suspicious_posts(df_processed)
synthetic_df = synthetic_generator.generate_complete_synthetic_posts(n_samples=75)

# Save synthetic posts to Excel and download
synthetic_df.to_excel('synthetic_posts.xlsx', index=False)
print("Synthetic posts saved to 'synthetic_posts.xlsx'")
from google.colab import files
files.download('synthetic_posts.xlsx')

print(f"Generated {len(synthetic_df)} complete synthetic fake posts based on suspicious post templates")

print("\n=== ALL SYNTHETIC Fake POSTS (Generated from Suspicious Post Templates) ===")
for i in range(len(synthetic_df)):
    sample = synthetic_df.iloc[i]
    print(f"\n=== SYNTHETIC Fake POST #{i+1} ===")
    print(f"Ad Text: {sample['Ad Text']}")
    print(f"Price: {sample['Price']}")
    print(f"Address: {sample['Address']}")
    print(f"Interaction: {sample['Interaction']}")
    print(f"Reasons: {sample['Reasons']}")

df_augmented = pd.concat([df_processed, synthetic_df], ignore_index=True)
df_augmented = preprocess_data(df_augmented)

print(f"\nOriginal dataset size: {len(df_processed)}")
print(f"Augmented dataset size: {len(df_augmented)}")
print(f"Fake posts in original: {len(df_processed[df_processed['Fake'] == 'Yes'])}")
print(f"Fake posts in augmented: {len(df_augmented[df_augmented['Fake'] == 'Yes'])}")

rf_detector = RFFakeDetector()

# Model Training and Evaluation
def evaluate_model(y_true, y_pred, model_name):
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()
    recall = recall_score(y_true, y_pred)
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    return {'Model': model_name, 'Recall (TPR)': recall, 'FPR': fpr}

def run_comprehensive_evaluation(df_orig, df_aug):
    results = []
    print("Evaluating Enhanced LLM (Statistical Weights) on Original Data...")
    llm_orig_pred, _, _ = enhanced_llm_detector.predict(df_orig)
    results.append(evaluate_model(df_orig['Fake_Binary'], llm_orig_pred, 'Enhanced LLM Statistical (Orig)'))

    print("Evaluating Enhanced LLM (Statistical Weights) on Augmented Data...")
    llm_aug_pred, _, _ = enhanced_llm_detector.predict(df_aug)
    results.append(evaluate_model(df_aug['Fake_Binary'], llm_aug_pred, 'Enhanced LLM Statistical (Aug)'))

    print("Training RF on Original Data...")
    rf_detector_orig = RFFakeDetector()
    rf_detector_orig.train(df_orig)
    rf_orig_pred, _ = rf_detector_orig.predict(df_orig)
    results.append(evaluate_model(df_orig['Fake_Binary'], rf_orig_pred, 'RF (Orig)'))

    print("Training RF on Augmented Data...")
    rf_detector_aug = RFFakeDetector()
    rf_detector_aug.train(df_aug)
    rf_aug_pred, _ = rf_detector_aug.predict(df_aug)
    results.append(evaluate_model(df_aug['Fake_Binary'], rf_aug_pred, 'RF (Aug)'))
    return results

results = run_comprehensive_evaluation(df_processed, df_augmented)
results_df = pd.DataFrame(results)
print("\n=== MODEL PERFORMANCE RESULTS ===")
print(results_df)

print("\n=== SAMPLE REASONING WITH STATISTICAL WEIGHTS ===")
sample_row = df_processed.iloc[0]
_, _, reasoning = enhanced_llm_detector.enhanced_reasoning(sample_row)
for step in reasoning[:8]:
    print(step)

# Visualizations
def create_performance_comparison_chart(results_df):
    fig = make_subplots(rows=1, cols=2, subplot_titles=('Recall (TPR) Comparison', 'FPR Comparison'))
    fig.add_trace(go.Bar(x=results_df['Model'], y=results_df['Recall (TPR)'], marker_color='#2ca02c'), row=1, col=1)
    fig.add_trace(go.Bar(x=results_df['Model'], y=results_df['FPR'], marker_color='#d62728'), row=1, col=2)
    fig.update_layout(height=500, width=1000, title_text="Enhanced Model Performance Comparison", title_x=0.5, showlegend=False)
    fig.update_yaxes(range=[0, 1])
    return fig

performance_fig = create_performance_comparison_chart(results_df)
performance_fig.show()

def create_enhanced_fake_comparison_chart(df_orig, df_aug):
    orig_counts = df_orig['Fake'].value_counts().reset_index()
    orig_counts['Dataset'] = 'Original'
    aug_counts = df_aug['Fake'].value_counts().reset_index()
    aug_counts['Dataset'] = 'Augmented (Complete Synthetic)'
    combined = pd.concat([orig_counts, aug_counts])
    combined.columns = ['Fake', 'Count', 'Dataset']
    fig = px.bar(combined, x='Dataset', y='Count', color='Fake', barmode='group', text='Count',
                 color_discrete_map={'Yes': 'red', 'No': 'green'}, title='Fake Post Distribution')
    fig.update_traces(texttemplate='%{text}', textposition='outside')
    fig.update_layout(height=500, width=800)
    return fig

fake_comp_fig = create_enhanced_fake_comparison_chart(df_processed, df_augmented)
fake_comp_fig.show()

def create_label_distribution_chart(df_orig, df_aug):
    orig_labels = df_orig['Label'].value_counts().reset_index()
    orig_labels['Dataset'] = 'Original'
    aug_labels = df_aug['Label'].value_counts().reset_index()
    aug_labels['Dataset'] = 'Augmented'
    combined = pd.concat([orig_labels, aug_labels])
    combined.columns = ['Label', 'Count', 'Dataset']
    fig = px.bar(combined, x='Dataset', y='Count', color='Label', barmode='group', text='Count',
                 title='Label Distribution')
    fig.update_traces(texttemplate='%{text}', textposition='outside')
    fig.update_layout(height=500, width=800)
    return fig

label_dist_fig = create_label_distribution_chart(df_processed, df_augmented)
label_dist_fig.show()

def create_confusion_matrices(df_orig, df_aug):
    fig = make_subplots(rows=2, cols=2, subplot_titles=('LLM Orig', 'LLM Aug', 'RF Orig', 'RF Aug'))
    llm_orig_pred, _, _ = enhanced_llm_detector.predict(df_orig)
    llm_aug_pred, _, _ = enhanced_llm_detector.predict(df_aug)
    rf_detector_orig = RFFakeDetector()
    rf_detector_orig.train(df_orig)
    rf_orig_pred, _ = rf_detector_orig.predict(df_orig)
    rf_detector_aug = RFFakeDetector()
    rf_detector_aug.train(df_aug)
    rf_aug_pred, _ = rf_detector_aug.predict(df_aug)

    for i, (pred, data) in enumerate([(llm_orig_pred, df_orig), (llm_aug_pred, df_aug), (rf_orig_pred, df_orig), (rf_aug_pred, df_aug)]):
        cm = confusion_matrix(data['Fake_Binary'], pred)
        fig.add_trace(go.Heatmap(z=cm, x=['Not Fake', 'Fake'], y=['Not Fake', 'Fake'], colorscale='Blues', showscale=False, text=cm, texttemplate="%{text}"),
                      row=(i//2)+1, col=(i%2)+1)
    fig.update_layout(height=800, width=800, title_text="Confusion Matrices", title_x=0.5)
    return fig

confusion_fig = create_confusion_matrices(df_processed, df_augmented)
confusion_fig.show()

def create_results_table(results_df):
    fig = go.Figure(data=[go.Table(header=dict(values=list(results_df.columns), fill_color='#1f77b4', align='center', font=dict(color='white', size=12)),
                                 cells=dict(values=[results_df[col] for col in results_df.columns], fill_color='lavender', align='center', format=[None, ".3f", ".3f"]))])
    fig.update_layout(title="Model Performance Comparison", title_x=0.5)
    return fig

results_table = create_results_table(results_df)
results_table.show()
